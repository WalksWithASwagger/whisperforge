"""
WhisperForge - AI-powered audio transcription and analysis tool.

This module contains the main Streamlit application code for WhisperForge,
integrating OpenAI's Whisper model with Notion for documentation.
"""

import streamlit as st
from streamlit.components.v1 import html
import tempfile
import os
from pathlib import Path
from openai import OpenAI
from datetime import datetime, timezone, timedelta
from notion_client import Client
import hashlib
import pickle
from dotenv import load_dotenv
import logging
import time

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('whisperforge.log')
    ]
)

# Load environment variables from .env file
load_dotenv()

# Initialize the OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Define a simple cache dictionary
cache = {}

def local_css():
    st.markdown("""
        <style>
        /* Main theme colors */
        :root {
            --main-bg-color: #000000;
            --text-color: #ffffff;
            --accent-color: #ff4d4d;
            --border-color: rgba(255,255,255,0.1);
            --secondary-bg: #1a1a1a;
            --hover-bg: #2a2a2a;
        }
        
        /* Global styles */
        .stApp {
            background-color: var(--main-bg-color);
            color: var(--text-color);
        }
        
        /* Sidebar styling */
        .css-1d391kg {  /* Sidebar */
            background-color: var(--main-bg-color);
        }
        
        /* Button styling */
        .stButton > button {
            width: 100%;
            background-color: var(--secondary-bg);
            color: var(--text-color);
            border: 1px solid var(--border-color);
            padding: 15px 20px;
            border-radius: 12px;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            margin: 5px 0;
            text-align: left;
        }
        
        .stButton > button:hover {
            background-color: var(--hover-bg);
            border-color: var(--accent-color);
        }
        
        /* Input fields */
        .stTextInput input {
            background-color: var(--secondary-bg);
            border: 1px solid var(--border-color);
            color: var(--text-color);
            border-radius: 12px;
            padding: 12px;
        }
        
        /* File uploader */
        .stFileUploader {
            background-color: var(--secondary-bg);
            border: 2px dashed var(--border-color);
            padding: 20px;
            border-radius: 12px;
            transition: all 0.3s ease;
        }
        
        .stFileUploader:hover {
            border-color: var(--accent-color);
        }
        
        /* Expander */
        .streamlit-expanderHeader {
            background-color: var(--secondary-bg);
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }
        
        .streamlit-expanderHeader:hover {
            background-color: var(--hover-bg);
        }
        
        /* Text areas */
        .stTextArea textarea {
            background-color: var(--secondary-bg);
            color: var(--text-color);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 12px;
        }
        
        /* Select boxes */
        .stSelectbox > div > div {
            background-color: var(--secondary-bg);
            border: 1px solid var(--border-color);
            border-radius: 12px;
        }
        
        /* Progress bars */
        .stProgress > div > div {
            background-color: var(--accent-color);
        }
        
        /* Success/Info messages */
        .stSuccess, .stInfo {
            background-color: var(--secondary-bg);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 10px;
        }
        </style>
    """, unsafe_allow_html=True)

def render_header():
    st.markdown('<div class="brand-header"><h1 class="brand-name">WhisperForge</h1></div>', unsafe_allow_html=True)

def render_footer():
    st.markdown("""
        <div class="footer">
            <p>Created by Kris Kr√ºg ¬© 2024 | WhisperForge.ai v0.1 alpha</p>
        </div>
    """, unsafe_allow_html=True)

def split_audio(file_path, chunk_length_ms):
    filename = os.path.splitext(os.path.basename(file_path))[0]
    st.info(f"Loading audio file: {filename}")
    
    # Use ffmpeg directly instead of pydub
    import subprocess
    import math
    
    # Get duration using ffprobe
    duration_cmd = [
        'ffprobe', 
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        file_path
    ]
    
    duration = float(subprocess.check_output(duration_cmd).decode('utf-8').strip())
    num_chunks = math.ceil(duration / (chunk_length_ms / 1000))
    
    chunk_files = []
    for i in range(num_chunks):
        start_time = i * chunk_length_ms / 1000
        chunk_file = f"whisperforge_{filename}_chunk_{i}.mp3"
        
        # Use ffmpeg to extract chunk
        subprocess.run([
            'ffmpeg',
            '-y',  # Overwrite output files
            '-i', file_path,
            '-ss', str(start_time),
            '-t', str(chunk_length_ms / 1000),
            '-acodec', 'libmp3lame',
            chunk_file
        ], stderr=subprocess.DEVNULL)
        
        chunk_files.append(chunk_file)
        st.info(f"Created chunk {i+1} of {num_chunks}")
    
    return chunk_files

def transcribe_chunks(chunk_files):
    transcriptions = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, chunk_file in enumerate(chunk_files):
        try:
            with open(chunk_file, "rb") as audio_file:
                # Update progress bar and status
                progress = (i + 1) / len(chunk_files)
                progress_bar.progress(progress)
                status_text.text(f"Transcribing: {i + 1}/{len(chunk_files)}")
                
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file
                )
                transcriptions.append(transcription.text)
                
                # Clean up chunk file after processing
                os.remove(chunk_file)
        except Exception as e:
            st.error(f"Error transcribing chunk {i+1}: {str(e)}")
    
    progress_bar.empty()
    status_text.empty()
    return transcriptions

def chunk_text(text, max_tokens=6000):
    """Split text into chunks that fit within token limits."""
    # Rough estimate: 1 token ‚âà 4 characters
    chunk_size = max_tokens * 4
    chunks = []
    
    # Split into paragraphs first
    paragraphs = text.split('\n')
    current_chunk = []
    current_length = 0
    
    for paragraph in paragraphs:
        if current_length + len(paragraph) > chunk_size:
            # Save current chunk and start new one
            chunks.append('\n'.join(current_chunk))
            current_chunk = [paragraph]
            current_length = len(paragraph)
        else:
            current_chunk.append(paragraph)
            current_length += len(paragraph)
    
    # Add the last chunk
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks

def setup_advanced_settings():
    """Setup advanced settings in sidebar."""
    with st.sidebar:
        with st.expander("Advanced Settings", expanded=False):
            chunk_size = st.slider(
                "Chunk Size (tokens)", 
                min_value=1000, 
                max_value=6000, 
                value=4000,
                help="Adjust the size of text chunks sent to GPT. Lower this if you get token limit errors."
            )
            
            show_chunks = st.checkbox(
                "Show Chunk Processing", 
                value=False,
                help="Show detailed progress of chunk processing"
            )
            
            return {
                "chunk_size": chunk_size,
                "show_chunks": show_chunks
            }

def process_with_gpt(text, mode, custom_prompt="", settings=None):
    """Process text with GPT, handling long content by chunking.
    
    Args:
        text (str): The input text to process
        mode (str): Processing mode ('summarize', 'extract insights', or 'custom instructions')
        custom_prompt (str, optional): Custom instructions for processing. Defaults to "".
        settings (dict, optional): Processing settings including:
            - chunk_size (int): Maximum tokens per chunk (default: 4000)
            - show_chunks (bool): Whether to show chunk processing progress (default: False)
    
    Returns:
        str: Processed text result, or None if processing fails
        
    Error Handling:
        - Displays warnings for large texts (>100k estimated tokens)
        - Provides actionable solutions for token limit errors
        - Shows detailed progress for chunk processing when enabled
    """
    if settings is None:
        settings = {"chunk_size": 4000, "show_chunks": False}
    
    try:
        chunks = chunk_text(text, max_tokens=settings["chunk_size"])
        
        # Add token size warning if text is very large
        total_chars = len(text)
        estimated_tokens = total_chars // 4  # Rough estimate
        if estimated_tokens > 100000:  # Arbitrary large number
            st.warning(f"‚ö†Ô∏è Large text detected (~{estimated_tokens:,} tokens). Processing may take longer.")
        
        all_results = []
        
        # Create a progress bar for chunks
        if settings["show_chunks"]:
            chunk_progress = st.progress(0)
            chunk_status = st.empty()
        
        for i, chunk in enumerate(chunks):
            try:
                if settings["show_chunks"]:
                    chunk_status.write(f"Processing chunk {i+1} of {len(chunks)}...")
                    chunk_progress.progress((i + 1) / len(chunks))
                
                # Handle custom instructions specially
                if mode == "custom instructions":
                    if custom_prompt.lower().strip() == "in spanish":
                        prompt = f"Translate the following text to Spanish:\n\n{chunk}"
                    else:
                        prompt = f"{custom_prompt}\n\n{chunk}"
                elif mode == "summarize":
                    prompt = f"Please summarize the following text. Focus on key points and main ideas:\n\n{chunk}"
                elif mode == "extract insights":
                    prompt = f"""Analyze the following text and provide:
                    **One-line Summary:**
                    **Key Themes:**
                    **Notable Points:**\n\n{chunk}"""
                
                response = client.chat.completions.create(
                    model="gpt-4",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant skilled in analyzing and processing text."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000
                )
                
                result = response.choices[0].message.content
                all_results.append(result)
                
            except Exception as chunk_error:
                if "maximum context length" in str(chunk_error).lower():
                    st.error(f"""
                        üö´ Token limit exceeded in chunk {i+1}!
                        Try one of these solutions:
                        1. Reduce the chunk size in Advanced Settings
                        2. Use a shorter custom prompt
                        3. Process a smaller portion of the text
                        
                        Technical details: {str(chunk_error)}
                    """)
                else:
                    st.error(f"Error processing chunk {i+1}: {str(chunk_error)}")
                return None
        
        # Clear chunk progress indicators
        if settings["show_chunks"]:
            chunk_status.empty()
            chunk_progress.empty()
        
        # Combine results
        combined_result = "\n\n".join(all_results)
        
        # For translation, we don't need a final summary
        if mode == "custom instructions" and custom_prompt.lower().strip() == "in spanish":
            return combined_result
        
        # Final summary if multiple chunks
        if len(chunks) > 1:
            if settings["show_chunks"]:
                st.write("Creating final summary...")
            
            final_summary = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant skilled in synthesizing information."},
                    {"role": "user", "content": f"Please provide a cohesive summary combining these sections:\n\n{combined_result}"}
                ],
                max_tokens=2000
            )
            return final_summary.choices[0].message.content
        
        return combined_result
        
    except Exception as e:
        logging.error(f"Error processing with GPT: {str(e)}")
        st.error(f"Error processing with GPT: {str(e)}")
        return None

def generate_title_summary_tags(text):
    """Generate a title, one-line summary, and 5 relevant tags using GPT."""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": """Generate:
                1. A compelling title that captures the essence (5-7 words)
                2. A one-line summary (max 15 words)
                3. Five relevant tags (single words or short phrases)

                Format as:
                TITLE: [title]
                SUMMARY: [one line]
                TAGS: [tag1], [tag2], [tag3], [tag4], [tag5]"""},
                {"role": "user", "content": text}
            ]
        )
        result = response.choices[0].message.content
        title = result.split("TITLE: ")[1].split("\n")[0].strip()
        summary = result.split("SUMMARY: ")[1].split("\n")[0].strip()
        tags = result.split("TAGS: ")[1].strip().split(", ")
        return title, summary, tags
    except Exception as e:
        st.error(f"Error generating title and summary: {str(e)}")
        return "Untitled Transcription", "[Summary generation failed]", ["transcription"]

def parse_processed_text(text):
    """Parse the processed text into sections."""
    sections = {
        "insights": [],
        "takeaways": [],
        "quotes": []
    }
    
    current_section = None
    for line in text.split('\n'):
        line = line.strip()
        if "1. Key Insights" in line:
            current_section = "insights"
        elif "2. Actionable Takeaways" in line:
            current_section = "takeaways"
        elif "3. Notable Quotes" in line:
            current_section = "quotes"
        elif line.startswith("- ") and current_section:
            sections[current_section].append(line[2:])  # Remove "- " prefix
            
    return sections

def send_to_notion(transcription, processed_text, notion_key, filename):
    """Send transcription and processed text to Notion with rich formatting."""
    try:
        notion = Client(auth=notion_key)
        database_id = "124c6f799a33806593b1c7afdc34ef94"
        
        # Generate title, summary, and tags using GPT
        title, summary, tags = generate_title_summary_tags(transcription)
        
        # Parse the processed text into sections
        sections = parse_processed_text(processed_text)
        
        # Create the page content with rich formatting
        children = [
            # Summary Callout with black square
            {
                "object": "block",
                "type": "callout",
                "callout": {
                    "rich_text": [{"text": {"content": summary}}],
                    "icon": {"emoji": "‚¨õ"}
                }
            },
            # Divider
            {"object": "block", "type": "divider", "divider": {}},
            
            # Key Insights Toggle
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"text": {"content": "Key Insights"}}],
                    "children": [
                        {
                            "object": "block",
                            "type": "bulleted_list_item",
                            "bulleted_list_item": {
                                "rich_text": [{"text": {"content": insight}}]
                            }
                        } for insight in sections["insights"]
                    ]
                }
            },
            
            # Actionable Takeaways Toggle
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"text": {"content": "Actionable Takeaways"}}],
                    "children": [
                        {
                            "object": "block",
                            "type": "bulleted_list_item",
                            "bulleted_list_item": {
                                "rich_text": [{"text": {"content": takeaway}}]
                            }
                        } for takeaway in sections["takeaways"]
                    ]
                }
            },
            
            # Notable Quotes Toggle
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"text": {"content": "Notable Quotes"}}],
                    "children": [
                        {
                            "object": "block",
                            "type": "quote",
                            "quote": {
                                "rich_text": [{"text": {"content": quote}}]
                            }
                        } for quote in sections["quotes"]
                    ]
                }
            },
            
            # Divider before transcription
            {"object": "block", "type": "divider", "divider": {}},
            
            # Full Transcription Toggle
            {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"text": {"content": "Full Transcription"}}],
                    "children": split_transcription_into_blocks(transcription)
                }
            }
        ]

        # Create the page with combined title and filename
        combined_title = f"{title} - {filename}"
        response = notion.pages.create(
            parent={"database_id": database_id},
            properties={
                "Name": {
                    "title": [{"text": {"content": combined_title}}]
                },
                "Created Date": {
                    "date": {
                        "start": datetime.now(tz=timezone(timedelta(hours=-8))).isoformat()
                    }
                },
                "Tags": {
                    "multi_select": [{"name": tag} for tag in tags] + [{"name": "transcription"}]
                }
            },
            children=children
        )
        
        return True, "Successfully sent to Notion!", f"https://notion.so/{response['id'].replace('-', '')}"
        
    except Exception as e:
        logging.error(f"Notion export error: {str(e)}")
        return False, f"Error sending to Notion: {str(e)}", None

def split_transcription_into_blocks(transcription, max_length=1800):
    """Split transcription into Notion blocks."""
    chunks = []
    words = transcription.split()
    current_chunk = []
    current_length = 0
    
    for word in words:
        if current_length + len(word) + 1 > max_length:
            chunks.append(" ".join(current_chunk))
            current_chunk = [word]
            current_length = len(word)
        else:
            current_chunk.append(word)
            current_length += len(word) + 1
            
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return [
        {
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{
                    "text": {"content": chunk}
                }]
            }
        }
        for chunk in chunks
    ]

def setup_notion_integration():
    st.sidebar.markdown("### Notion Integration")
    notion_key = st.sidebar.text_input(
        "Notion API Key", 
        value=os.getenv("NOTION_API_KEY", ""),
        type="password",
        help="Your Notion integration token"
    )
    return notion_key, "124c6f799a33806593b1c7afdc34ef94"

def cleanup_temp_files():
    """Clean up any temporary files created during processing."""
    try:
        for file in os.listdir():
            if file.startswith("whisperforge_") and file.endswith(".mp3"):
                os.remove(file)
    except Exception as e:
        st.error(f"Error cleaning up temporary files: {str(e)}")

def get_cache_key(file_path):
    """Generate a unique cache key based on the file content."""
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def load_cache():
    """Load cache from a file if it exists."""
    global cache
    if os.path.exists("cache.pkl"):
        with open("cache.pkl", "rb") as f:
            cache = pickle.load(f)
        logging.info("Cache loaded successfully.")
    else:
        logging.info("No cache file found. Starting with an empty cache.")

def save_cache():
    """Save cache to a file."""
    with open("cache.pkl", "wb") as f:
        pickle.dump(cache, f)
    logging.info(f"Cache saved successfully. Contains {len(cache)} items.")
    # Only rerun if not in the middle of processing
    if not st.session_state.get('processing_complete'):
        st.rerun()

def transcribe_with_cache(file_path):
    """Transcribe audio with caching and verification."""
    cache_key = get_cache_key(file_path)
    
    logging.info(f"File: {os.path.basename(file_path)}")
    logging.info(f"Cache key: {cache_key}")
    logging.info(f"Current cache size: {len(cache)} items")
    
    if cache_key in cache:
        cached_text = cache[cache_key]
        logging.info(f"Cache HIT! Text length: {len(cached_text)}")
        st.success(f"üéØ Using cached transcription! (Cache key: {cache_key[:8]}...)")
        
        # Add verification UI
        with st.expander("Verify cached content"):
            st.info("Cached transcription preview:")
            st.code(cached_text[:200] + "...")
        return cached_text
    
    logging.info("Cache MISS - Performing new transcription")
    transcription = transcribe_audio(file_path)
    cache[cache_key] = transcription
    save_cache()
    
    return transcription

def transcribe_audio(file_path):
    """Transcribe audio using OpenAI's Whisper model with progress updates."""
    logging.info(f"Starting transcription for file: {file_path}")
    
    # Get file size for progress estimation
    file_size = os.path.getsize(file_path)
    st.write(f"Processing {file_size/1024/1024:.1f}MB audio file...")
    
    # Create a progress placeholder
    progress_placeholder = st.empty()
    
    start_time = time.time()
    try:
        with open(file_path, "rb") as audio_file:
            # Show initial progress message
            progress_placeholder.info("Starting transcription... (this may take a few minutes)")
            
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
            
            # Show completion time
            duration = time.time() - start_time
            progress_placeholder.success(f"Transcription completed in {duration:.1f} seconds!")
            
            logging.info(f"Completed transcription for file: {file_path}")
            return response.text
            
    except Exception as e:
        progress_placeholder.error(f"Transcription error: {str(e)}")
        raise e

def clear_cache():
    """Clear the cache file."""
    if os.path.exists("cache.pkl"):
        os.remove("cache.pkl")
        st.sidebar.success("Cache cleared!")
        global cache
        cache = {}

def setup_cache_display():
    """Setup cache display in sidebar"""
    # Move cache controls into an expander
    with st.sidebar.expander("üîß Developer Tools", expanded=False):
        st.markdown("### üíæ Cache Status")
        
        # Force refresh cache status
        if os.path.exists("cache.pkl"):
            try:
                with open("cache.pkl", "rb") as f:
                    current_cache = pickle.load(f)
                    st.success(f"üì¶ Cache contains {len(current_cache)} items")
                    if st.checkbox("Show Cache Details"):
                        for key in current_cache.keys():
                            st.code(f"Cache Key: {key[:8]}...")
                    st.session_state.cache_initialized = True
            except Exception as e:
                st.warning(f"Error reading cache: {str(e)}")
        else:
            st.warning("üö´ No cache file exists")
            st.session_state.cache_initialized = False

        if st.button("üßπ Clear Cache"):
            if os.path.exists("cache.pkl"):
                os.remove("cache.pkl")
                st.success("Cache cleared!")
                st.session_state.cache_initialized = False
                st.rerun()

def create_download_button(text, filename, button_text):
    """Create a download button for text content."""
    try:
        # Create a temporary file with the content
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as tmp_file:
            tmp_file.write(text)
            tmp_file.flush()
            
            # Read the file in binary mode for download
            with open(tmp_file.name, 'rb') as f:
                return st.download_button(
                    label=button_text,
                    data=f.read(),
                    file_name=filename,
                    mime='text/plain'
                )
    except Exception as e:
        st.error(f"Error creating download button: {str(e)}")
        return None

def main():
    # Load cache at startup
    load_cache()
    
    # Setup cache display
    setup_cache_display()
    
    local_css()
    render_header()

    # Initialize session state variables if they don't exist
    if 'transcription_text' not in st.session_state:
        st.session_state.transcription_text = None
    if 'processed_text' not in st.session_state:
        st.session_state.processed_text = None

    # Sidebar configuration
    with st.sidebar:
        st.markdown("### Configuration")
        api_key = os.getenv("OPENAI_API_KEY")
        notion_key = None
        enable_notion = st.checkbox("Enable Notion Export")
        if enable_notion:
            notion_key = os.getenv("NOTION_API_KEY")

    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_file = st.file_uploader("Upload Audio File", type=['mp3', 'wav', 'm4a', 'ogg'])
        
    with col2:
        with st.expander("Advanced Settings"):
            chunk_length = st.slider("Chunk Length (min)", 1, 30, 5)
            language = st.selectbox("Language", ["Auto-detect", "English", "Spanish", "French", "German"])
    
    # Processing Mode Selection
    processing_mode = st.selectbox(
        "Processing Mode",
        ["Extract Insights", "Summarize", "Custom Instructions"],
        format_func=lambda x: x.title()
    )
    
    custom_prompt = ""
    if processing_mode == "Custom Instructions":
        custom_prompt = st.text_area("Custom Instructions", height=100)

    # Process button
    if uploaded_file:
        if st.button("Process Audio"):
            try:
                # Create status containers
                main_status = st.empty()
                progress_bar = st.progress(0)
                step_status = st.empty()
                
                # Step 1: Load File (10%)
                main_status.info("üìÇ Processing Audio File...")
                step_status.write("Step 1/4: Loading audio file")
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    audio_path = tmp_file.name
                progress_bar.progress(0.1)
                
                # Step 2: Transcribe (40%)
                step_status.write("Step 2/4: Transcribing audio")
                transcription = transcribe_with_cache(audio_path)
                if not transcription:
                    raise Exception("Transcription failed")
                st.session_state.transcription_text = transcription
                progress_bar.progress(0.4)
                
                # Step 3: Process with GPT (70%)
                step_status.write("Step 3/4: Processing with GPT")
                if processing_mode == "Custom Instructions":
                    advanced_settings = setup_advanced_settings()
                    processed_text = process_with_gpt(
                        transcription, 
                        processing_mode.lower(), 
                        custom_prompt,
                        settings=advanced_settings
                    )
                else:
                    processed_text = process_with_gpt(transcription, processing_mode.lower())
                if not processed_text:
                    raise Exception("GPT processing failed")
                st.session_state.processed_text = processed_text
                progress_bar.progress(0.7)
                
                # Step 4: Export to Notion (100%)
                step_status.write("Step 4/4: Exporting to Notion")
                if enable_notion and notion_key:
                    success, message, url = send_to_notion(
                        transcription,
                        processed_text,
                        notion_key,
                        uploaded_file.name
                    )
                    if success:
                        main_status.success(f"‚úÖ Complete! View in Notion: {url}")
                    else:
                        main_status.error(message)
                else:
                    main_status.success("‚úÖ Processing complete!")
                
                progress_bar.progress(1.0)
                step_status.empty()
                
            except Exception as e:
                main_status.error(f"Error: {str(e)}")
                logging.error(f"Processing error: {str(e)}")
            finally:
                cleanup_temp_files()

    # Show results if they exist in session state
    if st.session_state.transcription_text:
        col1, col2 = st.columns([6, 1])
        with col1:
            st.text_area("Full Transcription", st.session_state.transcription_text, height=300)
        with col2:
            create_download_button(
                st.session_state.transcription_text,
                "transcription.txt",
                "‚¨áÔ∏è Download Transcription"
            )
        
    if st.session_state.processed_text:
        col1, col2 = st.columns([6, 1])
        with col1:
            st.text_area("Processed Result", st.session_state.processed_text, height=300)
        with col2:
            create_download_button(
                st.session_state.processed_text,
                "processed_result.txt",
                "‚¨áÔ∏è Download Results"
            )

    render_footer()

if __name__ == "__main__":
    main()


